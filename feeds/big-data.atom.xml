<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Angel Llosá</title><link href="http://angel-llosa.com/" rel="alternate"></link><link href="http://angel-llosa.com/feeds/big-data.atom.xml" rel="self"></link><id>http://angel-llosa.com/</id><updated>2012-11-20T16:51:00+01:00</updated><entry><title>instalar Pig (de Hadoop) en ubuntu en modo multi-cluster</title><link href="http://angel-llosa.com/instalar-pig-de-hadoop-en-ubuntu-en-modo-multi-cluster.html" rel="alternate"></link><updated>2012-11-20T16:51:00+01:00</updated><author><name>Angel Llosá</name></author><id>tag:angel-llosa.com,2012-11-20:instalar-pig-de-hadoop-en-ubuntu-en-modo-multi-cluster.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Introducción&lt;/strong&gt;
&lt;a class="reference external" href="http://pig.apache.org/"&gt;Apache Pig&lt;/a&gt; es una plataforma para análisis de datos que se ejecuta
sobre &lt;a class="reference external" href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt;. Pig tiene un compilador con el cual genera secuencias
de programas con MapReduce, los cuales se ejecutan de forma paralela
sobre la infraestructura en la que estemos ejecutando Hadoop.&lt;/p&gt;
&lt;p&gt;Para verlo más claro, tenemos el siguiente ejemplo escrito en &lt;a class="reference external" href="http://pig.apache.org/docs/r0.8.0/piglatin_ref1.html"&gt;Pig
Latin&lt;/a&gt; y ejecutado sobre la consola &lt;a class="reference external" href="http://pig.apache.org/docs/r0.8.0/piglatin_ref2.html#run"&gt;grunt&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;grunt&amp;gt; &lt;span class="nv"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; load &lt;span class="s1"&gt;'passwd'&lt;/span&gt; using PigStorage&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;':'&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
grunt&amp;gt; &lt;span class="nv"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; foreach A generate &lt;span class="nv"&gt;$0&lt;/span&gt; as id&lt;span class="p"&gt;;&lt;/span&gt;
grunt&amp;gt; dump B&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ejecutando el código anterior por consola, lo que hacemos es:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Cargamos el fichero guardado llamado "passwd" (en este caso es un
fichero típico de linux /etc/passwd) y cargamos su contenido en A,
diferenciando los campos con el caracter ":" (PigStorage(':')).&lt;/li&gt;
&lt;li&gt;Para cada línea de A, volcamos el primer campo en B y lo llamamos
"id".&lt;/li&gt;
&lt;li&gt;Mostramos por pantalla el resultado.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Es posible utilizar cualquier función de tratamiento de datos
realizada por nosotros en java y empaquetada en un jar (siempre
siguiendo las especificaciones, claro) y utilizarla como en este caso se
ha utilizado &lt;em&gt;PigStorage&lt;/em&gt;. De esta manera, podemos diseñar cualquier
tipo de tratamiento que no se utilice en Pig Latin.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Instalación&lt;/strong&gt;
Se supone que ya hemos seguido los pasos de &lt;a class="reference external" href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/"&gt;cómo instalar hadoop en
Ubuntu (modo multi-node)&lt;/a&gt;. Una vez hecho esto nos descargaremos la
última versión de &lt;a class="reference external" href="http://hadoop.apache.org/pig/releases.html"&gt;Pig desde la página de apache&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lo descomprimiremos donde queramos (a partir de ahora &lt;em&gt;/path/pig&lt;/em&gt;) y
añadiremos al path del usuario hadoop el directorio de binarios
/path/pig/bin:
&lt;tt class="docutils literal"&gt;export &lt;span class="pre"&gt;PATH=/&amp;lt;my-path-to-pig&amp;gt;/pig-n.n.n/bin:$PATH&lt;/span&gt;&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Una vez realizado esto, añadiremos las siguientes variables locales al
fichero .bashrc necesarias para la ejecución de Pig. Entre ellas están
las necesarias para que sepa donde está instalado hadoop:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HADOOP_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/hadoop
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HADOOP_SLAVES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HADOOP_HOME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/conf/slaves
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HADOOP_CONF_DIR&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HADOOP_HOME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;/conf
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/path/pig/bin/:/usr/local/hadoop/bin/:&lt;span class="nv"&gt;$PATH&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;PIG_CLASSPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/hadoop/conf
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejecución por consola&lt;/strong&gt;
Existen 2 modos de ejecución, en modo local o mapreduce (cluster).
Para ejecutar en modo local ejecutamos lo siguiente sobre la linea de
comandos:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;pig &lt;span class="pre"&gt;-x&lt;/span&gt; local&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Y para el modo distribuido:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;pig&lt;/tt&gt;
o
&lt;tt class="docutils literal"&gt;pig &lt;span class="pre"&gt;-x&lt;/span&gt; mapreduce&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ejemplo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Para ver un ejemplo de ejecución y comprobar que todo funciona
correctamente, ejecutaremos lo siguiente sobre la consola grunt:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;grunt&amp;gt; &lt;span class="nv"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; load &lt;span class="s1"&gt;'passwd'&lt;/span&gt; using PigStorage&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;':'&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
grunt&amp;gt; &lt;span class="nv"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; foreach A generate &lt;span class="nv"&gt;$0&lt;/span&gt; as id&lt;span class="p"&gt;;&lt;/span&gt;
grunt&amp;gt; dump B&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Ejecución por script&lt;/strong&gt;
Para lanzar scripts sin la ayuda de la consola, guardaremos el código
anterior en un fichero llamado (por ejemplo) &lt;strong&gt;id.pig&lt;/strong&gt;. Lo lanzaremos
en modo local de la siguiente manera:
&lt;tt class="docutils literal"&gt;pig &lt;span class="pre"&gt;-x&lt;/span&gt; local id.pig&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Y para el modo distribuido:
&lt;tt class="docutils literal"&gt;pig id.pig&lt;/tt&gt;
o
&lt;tt class="docutils literal"&gt;pig &lt;span class="pre"&gt;-x&lt;/span&gt; mapreduce id.pig&lt;/tt&gt;&lt;/p&gt;
&lt;p&gt;Y poco más. Para más información sobre Pig, podemos ir al &lt;a class="reference external" href="http://pig.apache.org/docs/r0.8.0/index.html"&gt;sitio
oficial&lt;/a&gt; donde tenemos más información sobre el lenguaje &lt;a class="reference external" href="http://pig.apache.org/docs/r0.8.0/piglatin_ref1.html"&gt;Pig Latin&lt;/a&gt; o
modos de &lt;a class="reference external" href="http://pig.apache.org/docs/r0.8.0/cookbook.html"&gt;optimizar la paralelización del código que desarrollemos&lt;/a&gt;.&lt;/p&gt;
</summary><category term="tutorial hadoop"></category><category term="pig"></category><category term="Ubuntu"></category><category term="hadoop"></category></entry><entry><title>ejecutar un trabajo en hadoop (single-node)</title><link href="http://angel-llosa.com/ejecutar-un-trabajo-en-hadoop-single-node.html" rel="alternate"></link><updated>2011-02-15T10:42:00+01:00</updated><author><name>Angel Llosá</name></author><id>tag:angel-llosa.com,2011-02-15:ejecutar-un-trabajo-en-hadoop-single-node.html</id><summary type="html">&lt;p&gt;Ya vimos como &lt;a class="reference external" href="http://binaridev.es/2010/12/como-instalar-hadoop-en-ubuntu-modo-single-node/"&gt;instalar y configurar hadoop en nuestra máquina&lt;/a&gt;.
Ahora toca ejecutar algo para ver cómo funciona. Para ello, vamos a
probar uno de los &lt;a class="reference external" href="http://wiki.apache.org/hadoop/WordCount"&gt;ejemplos que vienen en hadoop&lt;/a&gt;, el cual nos calcula
las veces que aparece una palabra en los textos que le pasamos.&lt;/p&gt;
&lt;p&gt;Para ello, primero nos bajaremos los ficheros que vamos a tratar:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.gutenberg.org/etext/20417"&gt;The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.gutenberg.org/etext/5000"&gt;The Notebooks of Leonardo Da Vinci&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.gutenberg.org/etext/4300"&gt;Ulysses by James Joyce&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nos bajamos los ficheros y los guardamos, por ejemplo, en
&lt;tt class="docutils literal"&gt;/tmp/gutenberg&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;hadoop@ubuntu:~&lt;span class="nv"&gt;$ &lt;/span&gt;ls -l /tmp/gutenberg/
total 3592
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; hadoop hadoop &lt;span class="m"&gt;674425&lt;/span&gt; 2007-01-22 12:56 20417-8.txt
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; hadoop hadoop &lt;span class="m"&gt;1423808&lt;/span&gt; 2006-08-03 16:36 7ldvc10.txt
-rw-r--r-- &lt;span class="m"&gt;1&lt;/span&gt; hadoop hadoop &lt;span class="m"&gt;1561677&lt;/span&gt; 2004-11-26 09:48 ulyss12.txt
hadoop@ubuntu:~&lt;span class="err"&gt;$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Si no teníamos arrancado hadoop, lo arrancamos
&lt;tt class="docutils literal"&gt;con &lt;span class="pre"&gt;bin/start-all.sh&lt;/span&gt;&lt;/tt&gt;. Una vez arrancado, vamos a copiar los ficheros
descargados a nuestro sistema HDFS, para que pueda ser tratado por
hadoop:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;bin/hadoop dfs -copyFromLocal /tmp/gutenberg gutenberg
bin/hadoop dfs -ls
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ahora, ejecutaremos el trabajo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;jar hadoop-0.20.2-examples.jar wordcount gutenberg gutenberg-output
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Comprobamos que nos ha guardado el resultado en el directorio en HDFS
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gutenberg-output&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;bin/hadoop dfs -ls
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Para ver que hay dentro de los ficheros que nos ha generado podemos
ejecutar lo siguiente:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;bin/hadoop dfs -cat gutenberg-output/part-r-00000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Si queremos verlo desde local, primero lo copiaremos a algún destino
de nuestro sistema de ficheros:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;mkdir /tmp/gutenberg-output
bin/hadoop dfs -getmerge gutenberg-output /tmp/gutenberg-output
head /tmp/gutenberg-output/gutenberg-output
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Interfaz Web&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Para monitorizar los trabajos que lanzamos, Hadoop tiene definidas
unas interfaces web para ello.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://localhost:50030/"&gt;http://localhost:50030/&lt;/a&gt; – MapReduce job tracker&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://localhost:50060/"&gt;http://localhost:50060/&lt;/a&gt; – Task tracker&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://localhost:50070/"&gt;http://localhost:50070/&lt;/a&gt; – HDFS name node&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Son un poco simples, pero sirven de ayuda para ver el estado de nuestros
trabajos.&lt;/p&gt;
</summary><category term="tutorial hadoop"></category><category term="cloud"></category><category term="Linux"></category><category term="Ubuntu"></category><category term="hadoop"></category></entry><entry><title>los 4 secretos de facebook para escalar</title><link href="http://angel-llosa.com/los-4-secretos-de-facebook-para-escalar.html" rel="alternate"></link><updated>2011-02-13T17:49:00+01:00</updated><author><name>Angel Llosá</name></author><id>tag:angel-llosa.com,2011-02-13:los-4-secretos-de-facebook-para-escalar.html</id><summary type="html">&lt;p&gt;Ya hablamos en este blog sobre &lt;a class="reference external" href="http://angel-llosa.com/el-software-detras-de-facebook.html"&gt;Facebook y el software que existe
detrás&lt;/a&gt;. Todos conocemos sus apabullantes datos de de usuarios, páginas
vista, fotos, etc.&lt;/p&gt;
&lt;p&gt;El directo de ingeniería de Facebook, Aditya Agarwal, dio una
conferencia con el tema "&lt;a class="reference external" href="http://www.infoq.com/presentations/Scale-at-Facebook"&gt;Escalar en Facebook&lt;/a&gt;".  La presentación es
muy recomendable aunque dure una hora y sea en inglés.  Mezcla
arquitectura a nivel técnico y metodología.&lt;/p&gt;
&lt;p&gt;Hace un buen repaso sobre la tecnología que están utilizando
actualmente y cómo. En detalle lo que hablábamos en el post anterior:
&lt;a class="reference external" href="http://angel-llosa.com/el-software-detras-de-facebook.html"&gt;uso de PHP, memcache, MySQL, HipHop, etc&lt;/a&gt;.&lt;/p&gt;
&lt;div style="font-size: 14px; line-height: 25px;"&gt;&lt;p&gt;Una parte que me ha gustado mucho ha sido la de la cultura de empresa,
la cual se basa en 3 puntos principales:&lt;/p&gt;
&lt;/div&gt;&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Muévete rápido e innova&lt;/strong&gt;, no te preocupes en romper cosas.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Alcanzar gran impacto con equipos pequeños&lt;/strong&gt;. Se trabaja mejor, más
rapido y hay mejor comunicación.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Se valiente e innova&lt;/strong&gt;. Un poco extensión del primer punto.&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;&lt;p&gt;También es interesante cuando aconseja que antes de pensar en escalar,
hay que hacerlo funcionar, ya que no vas a tener millones de usuarios
para empezar. &lt;strong&gt;Es más importante sacar el producto al mercado rápido&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;&lt;p&gt;A nivel técnico, me quedo con:&lt;/p&gt;
&lt;/div&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Desventajas PHP: tiene coste de CPU alto. Lo pasan a C++ con HipHop.
Es curiosa la gráfica que muestra, donde aparece primero C++ y luego
Java como los más óptimos.&lt;/li&gt;
&lt;li&gt;Uso de servicios: usa el lenguaje, librerias y herramientas correctas
para la tarea que quieres (usan c++, java, erlang, python). Facebook
utiliza mucho la arquitectura orientada a servicios, pero no tiene
una herramienta única, sino que depende de lo que busque, utiliza lo
que necesita.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lo dicho, &lt;a class="reference external" href="http://www.infoq.com/presentations/Scale-at-Facebook"&gt;la conferencia no tiene desperdicio&lt;/a&gt;. 50 minutos de charla y
10 de preguntas.&lt;/p&gt;
</summary><category term="escalar"></category><category term="facebook"></category></entry><entry><title>el software detrás de Facebook</title><link href="http://angel-llosa.com/el-software-detras-de-facebook.html" rel="alternate"></link><updated>2011-02-13T11:58:00+01:00</updated><author><name>Angel Llosá</name></author><id>tag:angel-llosa.com,2011-02-13:el-software-detras-de-facebook.html</id><summary type="html">&lt;p&gt;Siempre he tenido curiosidad de Facebook. Te conectas y se carga todo
en apenas segundos. A día de hoy, Facebook tiene 500 millones de
usuarios activos. ¿Cómo pueden hacerlo para que vaya tan rápido?
Buscando he encontrado que, para soportar esto, en Facebook han tenido
que romper con la manera tradicional de servir contenido y evolucionar
hacia nuevas soluciones. A parte de Oracle RAC (&lt;a class="reference external" href="http://www.slideshare.net/ragho/hive-icde-2010"&gt;en esta presentación,
transparencia 11&lt;/a&gt;), el uso de soluciones de software libre es
apabullante.&lt;/p&gt;
&lt;p&gt;Los números que maneja actualmente son:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;570.000 millones&lt;/strong&gt; de páginas vistas al mes.&lt;/li&gt;
&lt;li&gt;Tiene más fotos que el resto de sítios de fotos combinados (incluido
Flickr).&lt;/li&gt;
&lt;li&gt;Se suben &lt;strong&gt;3.000 millones de fotos&lt;/strong&gt; todos los meses y sirve más de
&lt;strong&gt;1.2 millones&lt;/strong&gt; de estas por segundo.&lt;/li&gt;
&lt;li&gt;Todo esto lo manejan más de 30.000 servidores.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;¿Y cual es el software que ayuda a manejar todo esto?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aunque aún mantiene parte de su plataforma LAMP, ha ido evolucionando
creando un compilador propio de PHP, linux con kernels parcheados por
ellos mismos y llevando lógica de MySQL al propio servidor web.
Además de esto, se ayuda de:
&lt;strong&gt;Hadoop / Hive&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="image0" src="http://farm5.static.flickr.com/4067/4712028518_ed2144f29c_o.png"/&gt;&lt;/p&gt;
&lt;p&gt;Cómo no, Hive sobre Hadoop está teniendo cada vez más peso
en la arquitectura de Facebook. Al igual que Cassandra, son del proyecto
Apache y son de código abierto. Se ejecutan sobre Java y se utilizan
para el proceso de grandes cantidades de datos. Hive se ejecuta sobre
Hadoop, por lo que puede distribuir el trabajo en tantos servidores como
tenga el cluster. En este blog ya hablamos sobre &lt;a class="reference external" href="http://binaridev.es/2010/12/como-instalar-hadoop-en-ubuntu-modo-multi-node/"&gt;cómo instalar Hadoop
en una máquina Ubuntu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Haystack&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Facebook utiliza un servidor http que utiliza Haystack para guardar y
servir las fotos. Es un servidor adaptado y optimizado para este
propósito. Haystack ha sido desarrollado por Facebook, para migrar desde
NFS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Memcached&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://memcached.org/"&gt;Memcached&lt;/a&gt; es un sistema distribuido para cachear la información que
fluye entre el servidor de base de datos y el servidor web. Éste ha sido
modificado y adaptado por Facebook, siempre buscando las optimizaciones
que necesitaban.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HipHop&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://wiki.github.com/facebook/hiphop-php/"&gt;HipHop&lt;/a&gt;convierte PHP en C++, que luego es compilado para obtener
mejor rendimiento que en PHP nativo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BigPipe&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.facebook.com/notes/facebook-engineering/bigpipe-pipelining-web-pages-for-high-performance/389414033919"&gt;BigPipe&lt;/a&gt;es un servidor de páginas web que utiliza Facebook para
servir cada página en secciones llamadas "pagelets" para optimizar el
rendimiento. De esta manera se puede obtener en paralelo varias partes
de la misma página, aumentando la velocidad de carga. Si te fijas
mientras se carga la página que estás solicitando, las diferentes
secciones se cargan en tiempos distintos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cassandra&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Se utiliza para las búsquedas del Inbox. &lt;a class="reference external" href="http://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt;es una base de
datos NoSQL que fue desarrollada por Facebook y liberada posteriormente.
Actualmente forma parte del proyecto Apache.&lt;/p&gt;
&lt;p&gt;Como vemos, en Facebook se utiliza un compendio de herramientas de
software libre bastante punteras (no me refiero a LAMP) las cuales, o
las han desarrollado ellos, o las utilizan, mejoran y amplían de manera
activa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fuentes&lt;/strong&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://highscalability.com/blog/2010/6/22/exploring-the-software-behind-facebook-the-worlds-largest-si.html"&gt;Highscalability.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.infoq.com/presentations/Facebook-Moving-Fast-at-Scale"&gt;infoq.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.facebook.com/notes.php?id=9445547199"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</summary><category term="cloud"></category><category term="mysql"></category><category term="facebook"></category><category term="php"></category><category term="hadoop"></category></entry></feed>